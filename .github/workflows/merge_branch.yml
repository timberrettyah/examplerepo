name: GitHub Actions Demo
on:
  push:
    branches:
      - master
      - main

permissions:
  id-token: write
  contents: read

env:
  PYTHON_VERSION: 3.9.5
  DATABRICKS_RESOURCE_ID: 2ff814a6-3304-4ab8-85cb-cd0e6f879c1d

jobs:
  cicd-pipeline:
    environment: production

    runs-on: ubuntu-latest
    strategy:
      max-parallel: 4

    steps:
      - run: echo "The job was automatically triggered by a ${{ github.event_name }} event."
      - run: echo "This job is now running on a ${{ runner.os }} server hosted by GitHub!"
      - run: echo "The name of your branch is ${{ github.ref }} and your repository is ${{ github.repository }}."

      - name: Check out repository code
        uses: actions/checkout@v2

      - run: echo "The ${{ github.repository }} repository has been cloned to the runner."
      - run: echo "The workflow is now ready to test your code on the runner."

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: 'Az CLI login'
        uses: azure/login@v1
        with:
            client-id: ${{ secrets.AZURE_CLIENT_ID }}
            tenant-id: ${{ secrets.AZURE_TENANT_ID }}
            subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}

      - name: Install pip
        run: |
          python -m pip install --upgrade pip

      - name: List files in the repository
        run: |
          ls ${{ github.workspace }}

      - run: echo "This job's status is ${{ job.status }}."

      - run: sudo apt install -y graphviz graphviz-dev

      - run: echo "Install requirements"|
          python3 -m venv venv |
          . venv/bin/activate |
          python3 -m pip install -r requirements.txt

      - run: echo "Install additional packages"|
          . venv/bin/activate |
          python3 -m pip install dbConnect databricks-cli dbx delta-spark mlflow

      - name: Configure DBR CLI & test
        run: |
          export DATABRICKS_AAD_TOKEN=$(az account get-access-token --resource ${{ env.DATABRICKS_RESOURCE_ID }} | jq -r .accessToken)
          databricks configure --aad-token --host ${{ secrets.DATABRICKS_HOST }}

      - run: echo "Install package itself"|
          . venv/bin/activate |
          python3 -m pip install -e ".[dev]"

      - name: Run pre-commit hooks
        run: pre-commit run --all-file

      - run: |
            . venv/bin/activate |
            pytest tests --doctest-modules --junitxml=junit/test-results.xml --cov=. --cov-report=html

      - name: Archive code coverage results
        uses: actions/upload-artifact@v2
        with:
          name: code-coverage-report
          path: htmlcov/

      - name: Package the wheel
        run: |
          python3 setup.py sdist bdist_wheel

      - name: Jobless deployment (files only upload)
        run: |
          dbx deploy --job=test-simulated-data-etl-2.0 --files-only

      - name: Run the job in a jobless fashion
        run: |
          dbx launch --job=test-simulated-data-etl-2.0 --as-run-submit --trace
